# ğŸ¯ H-OOI: Automated Insight Engine
### Multi-Agent Data-to-Deck Pipeline with AI Intelligence

> **From raw data to executive-ready reports in under 60 seconds â€” no manual work, no dashboards, just actionable insights.**

---

## 1. The Problem

**Context:** In AdTech and enterprise analytics, reporting is broken. Teams spend 4-6 hours weekly on:
- Manually downloading CSVs from multiple platforms
- Taking screenshots of dashboards
- Copy-pasting numbers into PowerPoint
- Writing the same "insights" every week

**The Pain:** By the time a report reaches decision-makers, the data is already old. Problems go unnoticed for days.

> **My Solution:** **H-OOI** â€” an autonomous multi-agent system that ingests any data source (CSV, SQL, PDF) and generates complete executive reports with AI-driven narratives. Drop a file, get a deck.

---

## 2. What It Does

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   INGEST    â”‚â”€â”€â”€â”€â–¶â”‚   ANALYZE   â”‚â”€â”€â”€â”€â–¶â”‚   NARRATE   â”‚â”€â”€â”€â”€â–¶â”‚   EXPORT    â”‚
â”‚             â”‚     â”‚             â”‚     â”‚             â”‚     â”‚             â”‚
â”‚ CSV/SQL/PDF â”‚     â”‚ LangGraph   â”‚     â”‚ Executive   â”‚     â”‚ PPTX / PDF  â”‚
â”‚ Any source  â”‚     â”‚ ReAct Agent â”‚     â”‚ Narratives  â”‚     â”‚ Ready deck  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Input:** Raw data file (CSV, Excel, SQL query, PDF report)

**Output:** 
- Structured analysis with metrics, trends, anomalies
- Executive summary in natural language
- 5-slide presentation-ready content
- Downloadable PPTX/PDF report

---

## 3. Technical Architecture

### Multi-Agent Design (LangGraph)

I chose a **multi-agent architecture** over a single monolithic prompt because:
1. **Separation of concerns** â€” each agent has one job and does it well
2. **Debuggability** â€” I can trace exactly which agent made which decision
3. **Quality** â€” focused prompts produce better outputs than giant context dumps

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                         LANGGRAPH ORCHESTRATION                          â”‚
â”‚                                                                          â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚   â”‚  ANALYST AGENT (ReAct Pattern)                                   â”‚  â”‚
â”‚   â”‚                                                                   â”‚  â”‚
â”‚   â”‚  â€¢ Has access to Python/Pandas via execute_code tool             â”‚  â”‚
â”‚   â”‚  â€¢ Loops until comprehensive insights gathered                   â”‚  â”‚
â”‚   â”‚  â€¢ Saves: metrics, trends, anomalies, performers                 â”‚  â”‚
â”‚   â”‚  â€¢ Self-terminates when analysis complete                        â”‚  â”‚
â”‚   â”‚                                                                   â”‚  â”‚
â”‚   â”‚  Tools: execute_code, save_metric, save_trend, save_anomaly,    â”‚  â”‚
â”‚   â”‚         save_performer, save_chart_data, complete_analysis       â”‚  â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚                                    â”‚                                     â”‚
â”‚                                    â–¼                                     â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚   â”‚  NARRATOR AGENT (Sequential Pipeline)                            â”‚  â”‚
â”‚   â”‚                                                                   â”‚  â”‚
â”‚   â”‚  6 focused nodes, each with a specific writing task:            â”‚  â”‚
â”‚   â”‚  summary â†’ metrics â†’ performance â†’ trends â†’ risks â†’ recommendations â”‚
â”‚   â”‚                                                                   â”‚  â”‚
â”‚   â”‚  Output: Executive-ready narratives + slide content             â”‚  â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚                                    â”‚                                     â”‚
â”‚                                    â–¼                                     â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚   â”‚  REPORT BUILDER                                                  â”‚  â”‚
â”‚   â”‚                                                                   â”‚  â”‚
â”‚   â”‚  Generates: PPTX or PDF with charts and narratives              â”‚  â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚                                                                          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Hybrid LLM Strategy (Cost Optimization)

I implemented a **two-tier LLM approach** to balance quality and cost:

| Task | LLM | Why |
|------|-----|-----|
| PDF preprocessing | **Ollama (Local)** | Free, handles bulk text |
| Chunk summarization | **Ollama (Local)** | No quality loss for extraction |
| Data analysis | **GPT-4o-mini** | Needs reasoning for tool calls |
| Narrative generation | **GPT-4o-mini** | Quality matters for final output |

**Result:** ~70% cost reduction compared to all-cloud approach.

### PDF Processing Pipeline

For unstructured data (PDFs), I built a preprocessing pipeline that minimizes LLM token usage:

```
PDF Upload
    â”‚
    â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ pdfplumber extract  â”‚  â† Zero LLM cost
â”‚ â€¢ Text extraction   â”‚
â”‚ â€¢ Table detection   â”‚
â”‚ â€¢ Regex metrics     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â”‚
    â”œâ”€â”€ Has tables? â”€â”€â–¶ Convert to DataFrame â”€â”€â–¶ Analyst Agent
    â”‚
    â””â”€â”€ Text only? â”€â”€â–¶ Chunk + Summarize (Ollama) â”€â”€â–¶ Insights
```

---

## 4. Tech Stack

| Layer | Technology | Rationale |
|-------|------------|-----------|
| **API** | FastAPI | Async, auto-docs, type-safe |
| **Orchestration** | LangGraph | State machines, conditional routing, tool calling |
| **LLM (Cloud)** | OpenAI GPT-4o-mini | Best cost/quality ratio |
| **LLM (Local)** | Ollama + Llama 3.2 | Free preprocessing |
| **Data Processing** | Pandas | Industry standard, rich ecosystem |
| **PDF Extraction** | pdfplumber | Better table detection than PyPDF |
| **Report Generation** | python-pptx | Native PowerPoint creation |
| **Validation** | Pydantic v2 | Strict schemas, clear errors |

---

## 5. API Endpoints

```
# Data Ingestion
POST /ingest/csv          Upload CSV file
POST /ingest/excel        Upload Excel file  
POST /ingest/sql          Connect to SQL database
POST /ingest/url          Load from URL
POST /ingest/pdf          Upload and process PDF

# Analysis Pipeline (Sync)
POST /analyze/{id}        Run Analyst Agent only
POST /narrate/{id}        Run Analyst + Narrator Agents

# Report Generation (Async)
POST /generate/{id}       Queue full pipeline â†’ Returns job_id
GET  /jobs/{job_id}       Check job status + progress
GET  /download/{job_id}   Download generated PPTX/PDF

# Results
GET  /datasets            List loaded datasets
GET  /datasets/{id}       Get dataset info
GET  /reports/{id}        Get analysis report
```

---

## 6. Key Design Decisions

### Why LangGraph?

I needed the AI to:
- **Loop** until analysis is complete (not just run once)
- **Remember** insights across multiple steps
- **Choose different paths** based on data type (tables vs text)

LangGraph provides these capabilities out of the box.

### Why ReAct Pattern for Analysis?

I considered two approaches:

| Approach | Pros | Cons |
|----------|------|------|
| Pre-defined analysis | Fast, predictable | Misses dataset-specific insights |
| **ReAct (chosen)** | Adaptive, thorough | More LLM calls |

The ReAct pattern lets the LLM decide what to analyze based on the data it sees. For marketing data, it calculates CTR. For subscription data, it calculates churn. No hardcoding needed.

### Why Separate Narrator Agent?

I could have asked the Analyst to also write narratives, but:
1. **Token efficiency** â€” Analyst already uses context for tools
2. **Quality** â€” Writing is a different skill than analysis
3. **Modularity** â€” I can swap narrative styles without touching analysis

---

## 7. Challenges & Solutions

### Challenge 1: LLM Tool Call Loops

**Problem:** The Analyst Agent would sometimes loop forever, calling `execute_code` with increasingly random queries.

**Solution:** 
- Hard cap at 15 iterations
- `complete_analysis` tool that signals termination
- System prompt emphasizing efficiency

### Challenge 2: PDF Token Explosion

**Problem:** A 20-page PDF sent directly to GPT-4 would cost $0.50+ per request.

**Solution:**
- Extract tables with pdfplumber (zero LLM cost)
- Regex extraction for numbers/percentages
- Chunk text and summarize with local Ollama
- Only final synthesis uses cloud LLM

### Challenge 3: Consistent Output Schemas

**Problem:** LLM outputs were inconsistent â€” sometimes JSON, sometimes prose, sometimes missing fields.

**Solution:**
- Pydantic models for all inter-agent communication
- Explicit schema in prompts
- Fallback defaults for missing data

---

## 8. Project Structure

```
h-ooi/
â”œâ”€â”€ main.py              # FastAPI application + all endpoints
â”œâ”€â”€ agents.py            # Analyst Agent + Narrator Agent (LangGraph)
â”œâ”€â”€ pdf_processor.py     # PDF extraction + local LLM pipeline
â”œâ”€â”€ report_builder.py    # PPTX/PDF generation
â”œâ”€â”€ requirements.txt     # Dependencies
â”œâ”€â”€ Dockerfile           # Container definition
â”œâ”€â”€ docker-compose.yml   # Multi-service orchestration
â”œâ”€â”€ env.example          # Environment template
â”œâ”€â”€ sample_data.csv      # Demo AdTech dataset
â””â”€â”€ README.md            # Documentation
```

**Total:** ~1,500 lines of Python across 4 core files.

---

## 9. How to Run

```bash
# 1. Clone and setup
git clone https://github.com/demonzblood007/groundtruth-AI-Hackathon.git
cd groundtruth-AI-Hackathon
python -m venv venv
.\venv\Scripts\activate  # Windows
pip install -r requirements.txt

# 2. Configure environment
copy env.example .env
# Add your OPENAI_API_KEY to .env

# 3. (Optional) Start Ollama for local LLM
ollama pull llama3.2

# 4. Run the server
uvicorn main:app --reload --port 8000

# 5. Open API docs
# http://localhost:8000/docs
```

### Quick Test

```bash
# Upload sample data
curl -X POST "http://localhost:8000/ingest/csv" \
  -F "file=@sample_data.csv"

# Response: {"dataset_id": "abc123", ...}

# Run full pipeline
curl -X POST "http://localhost:8000/narrate/abc123"

# Response: Complete analysis + narratives + slides
```

---

## 10. Deployment

### Docker Compose (Production-Ready)

```bash
# Build and run all services
docker-compose up --build

# Access API
http://localhost:8000/docs
```

```yaml
# docker-compose.yml
services:
  app:
    build: .
    ports:
      - "8000:8000"
    environment:
      - OPENAI_API_KEY=${OPENAI_API_KEY}
      - REDIS_URL=redis://redis:6379
    depends_on:
      - redis

  worker:
    build: .
    command: celery -A worker worker --loglevel=info
    environment:
      - OPENAI_API_KEY=${OPENAI_API_KEY}
      - REDIS_URL=redis://redis:6379
    depends_on:
      - redis

  redis:
    image: redis:alpine
    ports:
      - "6379:6379"
```

### Async Job Queue (Redis + Celery)

For long-running analyses, jobs are processed asynchronously:

```
POST /generate/123  â†’  {"job_id": "abc", "status": "queued"}
GET  /jobs/abc      â†’  {"status": "processing", "progress": 60}
GET  /jobs/abc      â†’  {"status": "complete", "download_url": "/download/abc"}
```

**Why async?**
- API responds instantly (no 60s timeout)
- Multiple workers can process in parallel
- Jobs survive server restarts
- Progress tracking for large files

### Environment Variables

| Variable | Required | Description |
|----------|----------|-------------|
| `OPENAI_API_KEY` | Yes | OpenAI API key for GPT-4o-mini |
| `REDIS_URL` | No | Redis connection (default: localhost:6379) |
| `OLLAMA_BASE_URL` | No | Local Ollama URL (default: localhost:11434) |

---

## 11. Future Enhancements

- [ ] Streaming responses for real-time progress  
- [ ] Chart images embedded in slides
- [ ] Email delivery of final reports
- [ ] Scheduled recurring reports
- [ ] Multi-file comparison analysis

---

## 12. Why This Architecture Wins

| Criteria | How We Achieved It |
|----------|----------------|
| **Production-Ready** | FastAPI, error handling, Docker deployment |
| **Cost-Efficient** | Local LLM for preprocessing, cloud LLM only when needed |
| **Explainable** | Every insight is traceable to a specific analysis step |
| **Extensible** | Easy to add new agents, tools, or data sources |
| **Modern Stack** | LangGraph, tool calling, async processing |

---

*Built to demonstrate practical AI engineering â€” not just prompt wrapping, but real system design.*

